{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  BertModel, BertConfig,BertTokenizer\n",
    "from transformers import BertTokenizer,DataCollatorWithPadding, TrainingArguments\n",
    "from transformers import BertForTokenClassification,BertModel\n",
    "from transformers import Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source_sentence', 'sentences_to_compare'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_modal_inputs = {\n",
    "\"source_sentence\": ['杭州余杭东方未来学校附近世纪华联商场(金家渡北苑店)'],\n",
    "\"sentences_to_compare\": [[\n",
    "'良渚街道金家渡北苑42号世纪华联超市(金家渡北苑店)',\n",
    "'金家渡路金家渡中苑南区70幢金家渡中苑70幢',\n",
    "'金家渡路140-142号附近家家福足道(金家渡店)'\n",
    "]]\n",
    "}\n",
    "train_datasets = Dataset.from_dict(single_modal_inputs)\n",
    "train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings = tokenizer([example[\"source_sentence\"] for example in iter(train_datasets)], truncation=True, padding=True)\n",
    "train_encodings.keys()\n",
    "#train_encodings['label'] = train_datasets['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TextNet(nn.Module):\n",
    "    def __init__(self, code_len):\n",
    "        super(TextNet,self).__init__()\n",
    "        #modelconfig = BertConfig.from_pretrained(model_name)\n",
    "        self.textextract = BertModel.from_pretrained(model_name)\n",
    "        embedding_dim = self.textextract.config.hidden_size\n",
    "        print(embedding_dim)#768\n",
    "        print(self.textextract.config)\n",
    "        self.fc = nn.Linear(embedding_dim, code_len)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self,tokens, segments, input_masks):\n",
    "        out = self.textextract(tokens, token_type_ids=segments,\n",
    "                                 \t\tattention_mask=input_masks)\n",
    "        text_embeddings = out[0][:,0,:]#output[0](batch size, sequence length, model hidden dimension)这里表示取cls的embedding\n",
    "        features = self.fc(text_embeddings)\n",
    "        features=self.tanh(features)\n",
    "        return text_embeddings\n",
    "\n",
    "model = TextNet(256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = model(torch.tensor(train_encodings['input_ids']),torch.tensor( train_encodings['token_type_ids']),\n",
    "                      torch.tensor(train_encodings['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encodings = tokenizer([example[\"sentences_to_compare\"][0] for example in iter(train_datasets)], truncation=True, padding=True)\n",
    "test_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding2 = model(torch.tensor(test_encodings['input_ids']),torch.tensor( test_encodings['token_type_ids']),\n",
    "                      torch.tensor(test_encodings['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer([example[\"sentences_to_compare\"][1] for example in iter(train_datasets)], truncation=True, padding=True)\n",
    "test_encodings.keys()\n",
    "embedding3 = model(torch.tensor(test_encodings['input_ids']),torch.tensor( test_encodings['token_type_ids']),\n",
    "                      torch.tensor(test_encodings['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9055967626981778"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cos_similar(v1: list, v2: list):\n",
    "    num = float(np.dot(v1, v2))  # 向量点乘\n",
    "    denom = np.linalg.norm(v1) * np.linalg.norm(v2)  # 求模长的乘积\n",
    "    return 0.5 + 0.5 * (num / denom) if denom != 0 else 0\n",
    "get_cos_similar(list(embedding3.detach().numpy()[0]),list(embedding1.detach().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2e8f04d65c62c49dbc0f2adda76bbd103f82c3255fc54c7e3855bd92e2af645"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
