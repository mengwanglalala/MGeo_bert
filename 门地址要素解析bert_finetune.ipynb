{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict, IterableDataset, IterableDatasetDict\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b11d075f0280a41a\n",
      "Reusing dataset json (C:\\Users\\pc\\.cache\\huggingface\\datasets\\json\\default-b11d075f0280a41a\\0.0.0\\a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77154cb4384740a698cf146f45f21d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e0ddbe2a7a74f79e\n",
      "Reusing dataset json (C:\\Users\\pc\\.cache\\huggingface\\datasets\\json\\default-e0ddbe2a7a74f79e\\0.0.0\\a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0d0157b7f34880bbea8b38237198cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_datasets = load_dataset(\"json\", data_files=[\"./data/train.json\"])\n",
    "test_datasets = load_dataset(\"json\", data_files=[\"./data/val.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['浙', '江', '杭', '州', '市', '江', '干', '区', '九', '堡', '镇', '三', '村', '村', '一', '区'], 'ner_tags': ['B-prov', 'E-prov', 'B-city', 'I-city', 'E-city', 'B-district', 'I-district', 'E-district', 'B-town', 'I-town', 'E-town', 'B-community', 'I-community', 'E-community', 'B-poi', 'E-poi']}\n"
     ]
    }
   ],
   "source": [
    "for p in iter(train_datasets['train']):\n",
    "    print(p)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 1970\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,DataCollatorWithPadding, TrainingArguments\n",
    "from transformers import BertForTokenClassification,BertModel\n",
    "from transformers import Trainer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForTokenClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 假设训练数据集包含以下数据：\n",
    "# data = [(\"这是一个例子句子。\", ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']),\n",
    "#         (\"我来自北京市海淀区。\", ['O', 'O', 'B-city', 'B-district', 'O', 'O', 'O', 'O']),\n",
    "#         (\"我喜欢看《红楼梦》。\", ['O', 'O', 'O', 'O', 'O', 'O', 'B-poi', 'E-poi'])]\n",
    "\n",
    "# # 将标签转换为对应的ID\n",
    "# label2id = {'O': 0, 'B-city': 1, 'B-district': 2, 'B-poi': 3, 'E-poi': 4}\n",
    "# train_data = []\n",
    "# for text, labels in data:\n",
    "#     label_ids = [label2id[label] for label in labels]\n",
    "#     train_data.append((text, label_ids))\n",
    "# train_data\n",
    "\n",
    "\n",
    "# input_ids = torch.tensor(tokenizer.encode(\"哈喽，你好\")).unsqueeze(0)  # Batch size 1\n",
    "# labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\n",
    "# print(input_ids,labels)\n",
    "# outputs = model(input_ids, labels=labels)\n",
    "# loss, scores = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trainloader = torch.utils.data.DataLoader(train_datasets['train'],batch_size=64,shuffle=True,num_workers=2)\n",
    "# # testloader = torch.utils.data.DataLoader(testset,batch_size=64,shuffle=False,num_workers=2)\n",
    "# import torch\n",
    "# import torch.utils.data as Data\n",
    "# class MyDataSet(Data.Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         super(MyDataSet, self).__init__()\n",
    "#         self.tokens = data['tokens']\n",
    "#         self.labels = data['ner_tags']\n",
    "#     def __len__(self):\n",
    "#         return len(self.tokens)\n",
    "#     def __getitem__(self,idx):\n",
    "#         tokenizer.encode(self.tokens)\n",
    "#         return tokenizer.encode(self.tokens), [label2id[0][n] for n in self.labels]\n",
    "\n",
    "# def label2id_func(x):\n",
    "#     return label2id[0][x]\n",
    "# #atasetes = MyDataSet(train_datasets['train'])\n",
    "# encode_data = []\n",
    "# for i, data in enumerate(train_datasets['train']):\n",
    "#     tokens = data['tokens']\n",
    "#     ner_tags = data['ner_tags']\n",
    "    \n",
    "#     res = [tokenizer.encode(data[\"tokens\"]),[label2id[0][n] for n in data['ner_tags']] ]\n",
    "    \n",
    "#     print(tokens,ner_tags,res)\n",
    "#     break\n",
    "\n",
    "# list(map(label2id_func,data['ner_tags']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloader = torch.utils.data.DataLoader(train_datasets['train'],batch_size=64,shuffle=True,num_workers=2)\n",
    "# trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id={\n",
    "            \"B-assist\": 0,\n",
    "            \"B-cellno\": 1,\n",
    "            \"B-city\": 2,\n",
    "            \"B-community\": 3,\n",
    "            \"B-devzone\": 4,\n",
    "            \"B-distance\": 5,\n",
    "            \"B-district\": 6,\n",
    "            \"B-floorno\": 7,\n",
    "            \"B-houseno\": 8,\n",
    "            \"B-intersection\": 9,\n",
    "            \"B-poi\": 10,\n",
    "            \"B-prov\": 11,\n",
    "            \"B-road\": 12,\n",
    "            \"B-roadno\": 13,\n",
    "            \"B-subpoi\": 14,\n",
    "            \"B-town\": 15,\n",
    "            \"B-village_group\": 16,\n",
    "            \"E-assist\": 17,\n",
    "            \"E-cellno\": 18,\n",
    "            \"E-city\": 19,\n",
    "            \"E-community\": 20,\n",
    "            \"E-devzone\": 21,\n",
    "            \"E-distance\": 22,\n",
    "            \"E-district\": 23,\n",
    "            \"E-floorno\": 24,\n",
    "            \"E-houseno\": 25,\n",
    "            \"E-intersection\": 26,\n",
    "            \"E-poi\": 27,\n",
    "            \"E-prov\": 28,\n",
    "            \"E-road\": 29,\n",
    "            \"E-roadno\": 30,\n",
    "            \"E-subpoi\": 31,\n",
    "            \"E-town\": 32,\n",
    "            \"E-village_group\": 33,\n",
    "            \"I-assist\": 34,\n",
    "            \"I-cellno\": 35,\n",
    "            \"I-city\": 36,\n",
    "            \"I-community\": 37,\n",
    "            \"I-devzone\": 38,\n",
    "            \"I-distance\": 39,\n",
    "            \"I-district\": 40,\n",
    "            \"I-floorno\": 41,\n",
    "            \"I-houseno\": 42,\n",
    "            \"I-intersection\": 43,\n",
    "            \"I-poi\": 44,\n",
    "            \"I-prov\": 45,\n",
    "            \"I-road\": 46,\n",
    "            \"I-roadno\": 47,\n",
    "            \"I-subpoi\": 48,\n",
    "            \"I-town\": 49,\n",
    "            \"I-village_group\": 50,\n",
    "            \"O\": 51,\n",
    "            \"S-assist\": 52,\n",
    "            \"S-community\": 53,\n",
    "            \"S-district\": 54,\n",
    "            \"S-intersection\": 55,\n",
    "            \"S-poi\": 56\n",
    "        },\n",
    "num_labels= 57\n",
    "id2label={\n",
    "            \"0\": \"B-assist\",\n",
    "            \"1\": \"B-cellno\",\n",
    "            \"2\": \"B-city\",\n",
    "            \"3\": \"B-community\",\n",
    "            \"4\": \"B-devzone\",\n",
    "            \"5\": \"B-distance\",\n",
    "            \"6\": \"B-district\",\n",
    "            \"7\": \"B-floorno\",\n",
    "            \"8\": \"B-houseno\",\n",
    "            \"9\": \"B-intersection\",\n",
    "            \"10\": \"B-poi\",\n",
    "            \"11\": \"B-prov\",\n",
    "            \"12\": \"B-road\",\n",
    "            \"13\": \"B-roadno\",\n",
    "            \"14\": \"B-subpoi\",\n",
    "            \"15\": \"B-town\",\n",
    "            \"16\": \"B-village_group\",\n",
    "            \"17\": \"E-assist\",\n",
    "            \"18\": \"E-cellno\",\n",
    "            \"19\": \"E-city\",\n",
    "            \"20\": \"E-community\",\n",
    "            \"21\": \"E-devzone\",\n",
    "            \"22\": \"E-distance\",\n",
    "            \"23\": \"E-district\",\n",
    "            \"24\": \"E-floorno\",\n",
    "            \"25\": \"E-houseno\",\n",
    "            \"26\": \"E-intersection\",\n",
    "            \"27\": \"E-poi\",\n",
    "            \"28\": \"E-prov\",\n",
    "            \"29\": \"E-road\",\n",
    "            \"30\": \"E-roadno\",\n",
    "            \"31\": \"E-subpoi\",\n",
    "            \"32\": \"E-town\",\n",
    "            \"33\": \"E-village_group\",\n",
    "            \"34\": \"I-assist\",\n",
    "            \"35\": \"I-cellno\",\n",
    "            \"36\": \"I-city\",\n",
    "            \"37\": \"I-community\",\n",
    "            \"38\": \"I-devzone\",\n",
    "            \"39\": \"I-distance\",\n",
    "            \"40\": \"I-district\",\n",
    "            \"41\": \"I-floorno\",\n",
    "            \"42\": \"I-houseno\",\n",
    "            \"43\": \"I-intersection\",\n",
    "            \"44\": \"I-poi\",\n",
    "            \"45\": \"I-prov\",\n",
    "            \"46\": \"I-road\",\n",
    "            \"47\": \"I-roadno\",\n",
    "            \"48\": \"I-subpoi\",\n",
    "            \"49\": \"I-town\",\n",
    "            \"50\": \"I-village_group\",\n",
    "            \"51\": \"O\",\n",
    "            \"52\": \"S-assist\",\n",
    "            \"53\": \"S-community\",\n",
    "            \"54\": \"S-district\",\n",
    "            \"55\": \"S-intersection\",\n",
    "            \"56\": \"S-poi\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 8856\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = train_datasets['train']\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "(8856, 67) (8856, 67)\n"
     ]
    }
   ],
   "source": [
    "# 准备训练数据\n",
    "train_encodings = tokenizer([''.join(data[\"tokens\"]) for data in iter(tokenized_datasets)], truncation=True, padding=True)\n",
    "max_len = len(train_encodings['input_ids'][0])\n",
    "print(max_len)\n",
    "train_labels = {\"labels\": [ [label2id[0]['O']] + [label2id[0][tag] for tag in data[\"ner_tags\"]] \n",
    "                 + [label2id[0]['O']]*(max_len-len(data[\"ner_tags\"])-1) if len(data[\"ner_tags\"]) <= max_len-2 else \n",
    "                 [label2id[0]['O']] + [label2id[0][tag] for tag in data[\"ner_tags\"][:max_len-2]] + [label2id[0]['O']]\n",
    "                  for data in iter(tokenized_datasets)]}#encode后会加S和E以及padding，label要保证长度一致\n",
    "\n",
    "print(np.array(train_labels['labels']).shape, np.array(train_encodings['input_ids']).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_encodings['labels'] = train_labels['labels']\n",
    "\n",
    "# #attention_mask = attention_mask#torch.tensor([[1] * (len(tokenized_text[:max_len])+2) + [0] * (max_len - len(tokenized_text[:max_len])) for tokenized_text in iter(tokenized_datasets)])\n",
    "# print(np.array(train_encodings.attention_mask).shape)\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# # 将输入数据转换为TensorDataset类型，用于训练和评估\n",
    "dataset = TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), torch.tensor(train_labels['labels'] ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x229f16ed988>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将数据加载到DataLoader中，用于批量训练和评估\n",
    "BATCH_SIZE = 64\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 初始化BERT模型\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-chinese',num_labels = len(label2id[0]))\n",
    "# model = nn.DataParallel(model)#多卡训练\n",
    "# model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "[1,    10] loss: 1.63724\n",
      "[1,    20] loss: 0.72477\n",
      "[1,    30] loss: 0.45325\n",
      "[1,    40] loss: 0.31002\n",
      "[1,    50] loss: 0.24236\n",
      "[1,    60] loss: 0.19151\n",
      "[1,    70] loss: 0.16512\n",
      "[1,    80] loss: 0.15471\n",
      "[1,    90] loss: 0.16183\n",
      "[1,   100] loss: 0.13879\n",
      "[1,   110] loss: 0.12499\n",
      "[1,   120] loss: 0.10749\n",
      "[1,   130] loss: 0.12143\n",
      "Epoch: 2\n",
      "[2,    10] loss: 0.19020\n",
      "[2,    20] loss: 0.09459\n",
      "[2,    30] loss: 0.09590\n",
      "[2,    40] loss: 0.08866\n",
      "[2,    50] loss: 0.09184\n",
      "[2,    60] loss: 0.07984\n",
      "[2,    70] loss: 0.08518\n",
      "[2,    80] loss: 0.08101\n",
      "[2,    90] loss: 0.09260\n",
      "[2,   100] loss: 0.07934\n",
      "[2,   110] loss: 0.06723\n",
      "[2,   120] loss: 0.07133\n",
      "[2,   130] loss: 0.07869\n",
      "Epoch: 3\n",
      "[3,    10] loss: 0.12715\n",
      "[3,    20] loss: 0.06303\n",
      "[3,    30] loss: 0.05643\n",
      "[3,    40] loss: 0.05212\n",
      "[3,    50] loss: 0.05586\n",
      "[3,    60] loss: 0.05877\n",
      "[3,    70] loss: 0.05257\n",
      "[3,    80] loss: 0.05080\n",
      "[3,    90] loss: 0.04876\n",
      "[3,   100] loss: 0.05420\n",
      "[3,   110] loss: 0.05298\n",
      "[3,   120] loss: 0.05332\n",
      "[3,   130] loss: 0.05736\n",
      "Epoch: 4\n",
      "[4,    10] loss: 0.08276\n",
      "[4,    20] loss: 0.04381\n",
      "[4,    30] loss: 0.03959\n",
      "[4,    40] loss: 0.03400\n",
      "[4,    50] loss: 0.03711\n",
      "[4,    60] loss: 0.03675\n",
      "[4,    70] loss: 0.04294\n",
      "[4,    80] loss: 0.03342\n",
      "[4,    90] loss: 0.04231\n",
      "[4,   100] loss: 0.03675\n",
      "[4,   110] loss: 0.03914\n",
      "[4,   120] loss: 0.03521\n",
      "[4,   130] loss: 0.03747\n",
      "Epoch: 5\n",
      "[5,    10] loss: 0.07363\n",
      "[5,    20] loss: 0.03107\n",
      "[5,    30] loss: 0.02597\n",
      "[5,    40] loss: 0.02844\n",
      "[5,    50] loss: 0.02919\n",
      "[5,    60] loss: 0.02773\n",
      "[5,    70] loss: 0.02778\n",
      "[5,    80] loss: 0.03155\n",
      "[5,    90] loss: 0.03263\n",
      "[5,   100] loss: 0.02826\n",
      "[5,   110] loss: 0.02986\n",
      "[5,   120] loss: 0.03137\n",
      "[5,   130] loss: 0.02981\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "# 定义优化器和学习率\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "running_loss = 0\n",
    "# 训练模型\n",
    "EPOCHS =5\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    for i,batch in enumerate(dataloader):\n",
    "        \n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        # 计算模型的损失\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n",
    "        loss = outputs[0]#.mean(dim=0)# 多GPU训练需要加.mean(dim=0)，否则直接使用outputs即可\n",
    "\n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "      \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.item()\n",
    "     \n",
    "        if i%10 == 9:\n",
    "            print('[%d, %5d] loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10.0))\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前计算机有2个GPU设备\n",
      "设备0的名称为：Quadro RTX 4000\n",
      "设备1的名称为：Quadro RTX 4000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 获取GPU设备数量\n",
    "num_devices = torch.cuda.device_count()\n",
    "print(\"当前计算机有{}个GPU设备\".format(num_devices))\n",
    "\n",
    "# 获取每个GPU设备的名称\n",
    "for i in range(num_devices):\n",
    "    device_name = torch.cuda.get_device_name(i)\n",
    "    print(\"设备{}的名称为：{}\".format(i, device_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([[10.9694,  9.8041,  9.8926,  ..., 12.4295, 12.3193, 12.3640],\n",
      "        [11.3796, 10.7151, 10.7688,  ..., 12.5885, 12.5957, 12.5869],\n",
      "        [11.2269, 11.0188, 10.8809,  ..., 12.5843, 12.5967, 12.5834],\n",
      "        ...,\n",
      "        [11.2177, 10.6192, 11.0087,  ..., 12.5550, 12.5434, 12.5630],\n",
      "        [11.3084, 10.8719, 10.4875,  ..., 12.4627, 12.4483, 12.4550],\n",
      "        [10.9361, 10.9430, 10.5469,  ..., 12.4301, 12.4269, 12.4429]],\n",
      "       device='cuda:0'),\n",
      "indices=tensor([[51, 10, 44,  ..., 51, 51, 51],\n",
      "        [51,  2, 19,  ..., 51, 51, 51],\n",
      "        [51,  2, 36,  ..., 51, 51, 51],\n",
      "        ...,\n",
      "        [51,  6, 40,  ..., 51, 51, 51],\n",
      "        [51, 11, 45,  ..., 51, 51, 51],\n",
      "        [51, 11, 45,  ..., 51, 51, 51]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i,batch in enumerate(dataloader):\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        # 计算模型的损失\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n",
    "        loss = outputs[0].mean(dim=0)# 多GPU训练需要加.mean(dim=0)，否则直接使用outputs即可\n",
    "        break\n",
    "print(torch.max(outputs[1],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 67])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 67, 57])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].shape#torch.max(outputs[1],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 67])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,pre = torch.max(outputs[1],2)\n",
    "pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[51, 10, 44,  ..., 51, 51, 51],\n",
       "        [51,  2, 19,  ..., 51, 51, 51],\n",
       "        [51,  2, 36,  ..., 51, 51, 51],\n",
       "        ...,\n",
       "        [51,  6, 40,  ..., 51, 51, 51],\n",
       "        [51, 11, 45,  ..., 51, 51, 51],\n",
       "        [51, 11, 45,  ..., 51, 51, 51]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[51, 10, 44,  ..., 51, 51, 51],\n",
       "        [51,  2, 19,  ..., 51, 51, 51],\n",
       "        [51,  2, 36,  ..., 51, 51, 51],\n",
       "        ...,\n",
       "        [51,  6, 40,  ..., 51, 51, 51],\n",
       "        [51, 11, 45,  ..., 51, 51, 51],\n",
       "        [51, 11, 45,  ..., 51, 51, 51]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 1970\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_datasets = test_datasets['train']\n",
    "val_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "(1970, 44) (1970, 44)\n"
     ]
    }
   ],
   "source": [
    "# 准备训练数据\n",
    "test_encodings = tokenizer([''.join(data[\"tokens\"]) for data in iter(val_datasets)], truncation=True, padding=True)\n",
    "max_len = len(test_encodings['input_ids'][0])\n",
    "print(max_len)\n",
    "test_labels = {\"labels\": [ [label2id[0]['O']] + [label2id[0][tag] for tag in data[\"ner_tags\"]] \n",
    "                 + [label2id[0]['O']]*(max_len-len(data[\"ner_tags\"])-1) if len(data[\"ner_tags\"]) <= max_len-2 else \n",
    "                 [label2id[0]['O']] + [label2id[0][tag] for tag in data[\"ner_tags\"][:max_len-2]] + [label2id[0]['O']]\n",
    "                  for data in iter(val_datasets)]}#encode后会加S和E以及padding，label要保证长度一致\n",
    "\n",
    "print(np.array(test_labels['labels']).shape, np.array(test_encodings['input_ids']).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x229f16ed988>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "test_encodings['labels'] = test_labels['labels']\n",
    "\n",
    "# #attention_mask = attention_mask#torch.tensor([[1] * (len(tokenized_text[:max_len])+2) + [0] * (max_len - len(tokenized_text[:max_len])) for tokenized_text in iter(tokenized_datasets)])\n",
    "# print(np.array(train_encodings.attention_mask).shape)\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# # 将输入数据转换为TensorDataset类型，用于训练和评估\n",
    "test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_encodings['attention_mask']), torch.tensor(test_labels['labels'] ))\n",
    "# 将数据加载到DataLoader中，用于批量训练和评估\n",
    "BATCH_SIZE = 64\n",
    "testloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1970, 44)\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "test_labels = []\n",
    "res = []\n",
    "with torch.no_grad():\n",
    "    for i,batch in enumerate(testloader):\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        # 计算模型的损失\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n",
    "        loss = outputs[0].mean(dim=0)# 多GPU训练需要加.mean(dim=0)，否则直接使用outputs即可\n",
    "        total_loss += loss.item()\n",
    "        _,tmp_res = torch.max(outputs[1],2)\n",
    "        res.append(tmp_res.cpu().numpy())\n",
    "        test_labels.append(batch_labels.cpu().numpy())\n",
    "\n",
    "    total_loss = total_loss/(BATCH_SIZE*i)\n",
    "print(np.concatenate(res,axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51, 15, 44, ..., 51, 51, 51],\n",
       "       [51, 11, 45, ..., 51, 51, 51],\n",
       "       [51,  6, 23, ..., 51, 51, 51],\n",
       "       ...,\n",
       "       [51, 12, 46, ..., 51, 51, 51],\n",
       "       [51, 12, 46, ..., 51, 51, 51],\n",
       "       [51,  6, 40, ..., 51, 51, 51]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(res,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51,  6, 23, ..., 51, 51, 51],\n",
       "       [51, 11, 45, ..., 51, 51, 51],\n",
       "       [51, 15, 32, ..., 51, 51, 51],\n",
       "       ...,\n",
       "       [51, 12, 46, ..., 51, 51, 51],\n",
       "       [51, 12, 46, ..., 51, 51, 51],\n",
       "       [51,  6, 40, ..., 51, 51, 51]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(test_labels,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002326412700737516"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct =(np.concatenate(res,axis=0) == np.concatenate(test_labels,axis=0)).sum().item()/(1970*44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970\n"
     ]
    }
   ],
   "source": [
    "len_matric = []\n",
    "for data in iter(test_datasets['train']):\n",
    "    len_matric.append(len(data['tokens']))\n",
    "print(len(len_matric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9657821873557915"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res_src = [ data[1: min(len_matric[i], max_len-2)]\n",
    "                  for i,data in enumerate(np.concatenate(res,axis=0))]#encode后会加S和E以及padding，label要保证长度一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_src = [ data[1: min(len_matric[i], max_len-2)]\n",
    "                  for i,data in enumerate(np.concatenate(test_labels,axis=0))]#encode后会加S和E以及padding，label要保证长度一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8799083868245788"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct =(np.concatenate(test_res_src,axis=0) == np.concatenate(test_label_src,axis=0)).sum().item()/sum(len_matric)\n",
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss: 0.05307\n",
    "correct: 0.876\n",
    "epoch: 3\n",
    "opt: Adam\n",
    "lr: 5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用tranformer库中的Trainer函数实现微调\n",
    "- 注意，前面使用的时Tensordatasets, 在使用Trainer时我们需要传输如Dataset类，需要自定义个类转化一下\n",
    "> Dataset是PyTorch中的一个抽象类，用于表示数据集。它定义了一个标准的接口，使得我们可以对各种不同格式的数据进行统一的处理。Dataset类本身不存储任何数据，而是提供了一种统一的方法来访问数据。\n",
    "\n",
    "> TensorDataset是Dataset类的一种实现，它接受多个张量作为输入，每个张量的第一个维度应该相同，用于表示样本数。TensorDataset将这些张量打包成一个元组，并将每个元组作为一个样本。因此，它适用于对张量数据进行训练和测试。\n",
    "\n",
    "> Dataset和TensorDataset的区别在于它们所接受的数据类型不同。Dataset可以接受各种不同类型的数据，例如文本、图像、音频等，而TensorDataset只能接受张量数据。此外，Dataset通常需要实现__getitem__和__len__方法，以便可以对数据进行索引和获取数据集的长度，而TensorDataset已经默认实现了这些方法。\n",
    "\n",
    "> 给定这个数据集描述 Dataset({ features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'], num_rows: 3668 })，如果使用TensorDataset类来表示该数据集，需要先将这些特征和标签数据都转换为张量格式。如果使用Dataset类来表示该数据集，则可以保留原始数据的格式，并将其放入一个列表中，每个元素对应一个样本。在需要访问数据时，使用相应的数据处理方法将数据转换为模型可以处理的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "# class MyDataSet(Dataset):\n",
    "#     def __init__(self, datasets):\n",
    "#         super(MyDataSet,self).__init__()\n",
    "#         self.data = []\n",
    "#         for i, tmp_data in enumerate(iter(datasets)):\n",
    "#             item = {}\n",
    "#             item['input_ids'] = tmp_data[0]\n",
    "#             item['attention_mask'] = tmp_data[1]\n",
    "#             item['labels'] = tmp_data[2]\n",
    "#             self.data.append(item)\n",
    "#     def __len__(self):\n",
    "#         return dataset.__len__()\n",
    "#     def __getitem__(self, index):\n",
    "#         item = self.data[index]\n",
    "#         return item\n",
    "# MyDataSet(dataset).data\n",
    "dataset_dic = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": [v[0] for v in iter(dataset)],\n",
    "        \"attention_mask\": [v[1] for v in iter(dataset)],\n",
    "        \"labels\": [v[2] for v in iter(dataset)],\n",
    "    }\n",
    ")\n",
    "\n",
    "test_dataset_dic = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": [v[0] for v in iter(test_dataset)],\n",
    "        \"attention_mask\": [v[1] for v in iter(test_dataset)],\n",
    "        \"labels\": [v[2] for v in iter(test_dataset)],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# (可选)将多个Dataset合并为一个DatasetDict\n",
    "dataset_all = DatasetDict({\"train\": dataset_dic, \"valid\": test_dataset_dic})\n",
    "\n",
    "dataset_all['train'][1].keys()\n",
    "# for v in iter(dataset):\n",
    "#     print(v)\n",
    "#     break\n",
    "# data = []\n",
    "# for i, tmp_data in enumerate(iter(dataset)):\n",
    "#     item = {}\n",
    "#     item['input_ids'] = tmp_data[0]\n",
    "#     item['attention_mask'] = tmp_data[1]\n",
    "#     item['labels'] = tmp_data[2]\n",
    "#     data.append(item)\n",
    "\n",
    "#     if i>3:\n",
    "#         break\n",
    "# print(data)\n",
    "#iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1970"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_all['valid'].num_rows\n",
    "# def validate_dataset(dataset):\n",
    "#     for i, example in enumerate(dataset):\n",
    "#         if isinstance(example, dict):\n",
    "#             if \"input_ids\" not in example:\n",
    "#                 raise ValueError(f\"Example {i} does not contain 'input_ids'.\")\n",
    "#             if \"attention_mask\" not in example:\n",
    "#                 raise ValueError(f\"Example {i} does not contain 'attention_mask'.\")\n",
    "#             if \"labels\" not in example:\n",
    "#                 raise ValueError(f\"Example {i} does not contain 'label'.\")\n",
    "#             if len(example[\"input_ids\"]) != len(example[\"attention_mask\"]) or len(example[\"input_ids\"]) != len(example[\"labels\"]):\n",
    "#                 raise ValueError(f\"Example {i} has mismatched input_ids, attention_mask, or label.\")\n",
    "#         elif isinstance(example, tuple):\n",
    "#             if len(example) != 3:\n",
    "#                 raise ValueError(f\"Example {i} does not have 3 elements.\")\n",
    "#             if not isinstance(example[0], torch.Tensor) or not isinstance(example[1], torch.Tensor) or not isinstance(example[2], torch.Tensor):\n",
    "#                 raise ValueError(f\"Example {i} is not a tuple of tensors.\")\n",
    "#             if example[0].size() != example[1].size() or example[0].size() != example[2].size():\n",
    "#                 raise ValueError(f\"Example {i} has mismatched tensor sizes.\")\n",
    "#         else:\n",
    "#             raise ValueError(f\"Example {i} is not a recognized data type.\")\n",
    "#     print(\"Dataset validation successful!\")\n",
    "# validate_dataset(dataset_all['valid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print(dataset,test_dataset)\n",
    "# 注意，这里不要使用多卡训练！\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset = dataset_all[\"train\"],\n",
    "    eval_dataset = dataset_all[\"valid\"],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftInstall\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 8856\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1662\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091a8b6123c642babace8c38eeffd969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SoftInstall\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\SoftInstall\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "Saving model checkpoint to test-trainer\\checkpoint-500\n",
      "Configuration saved in test-trainer\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0531, 'learning_rate': 3.495788206979543e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test-trainer\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in test-trainer\\checkpoint-500\\special_tokens_map.json\n",
      "D:\\SoftInstall\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\SoftInstall\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1970\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472417ff7b48429790880eb39403e38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15128105878829956, 'eval_runtime': 13.9193, 'eval_samples_per_second': 141.53, 'eval_steps_per_second': 8.908, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-trainer\\checkpoint-1000\n",
      "Configuration saved in test-trainer\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0361, 'learning_rate': 1.9915764139590855e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test-trainer\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in test-trainer\\checkpoint-1000\\special_tokens_map.json\n",
      "D:\\SoftInstall\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\SoftInstall\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1970\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f4ce8c1c454f9fbdf58854e2dfe385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16626973450183868, 'eval_runtime': 14.729, 'eval_samples_per_second': 133.75, 'eval_steps_per_second': 8.419, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-trainer\\checkpoint-1500\n",
      "Configuration saved in test-trainer\\checkpoint-1500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0249, 'learning_rate': 4.873646209386281e-06, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test-trainer\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in test-trainer\\checkpoint-1500\\special_tokens_map.json\n",
      "D:\\SoftInstall\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\SoftInstall\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1970\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26f449c11194c46ad5fb65eb2b1ffd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16120123863220215, 'eval_runtime': 21.0993, 'eval_samples_per_second': 93.368, 'eval_steps_per_second': 5.877, 'epoch': 3.0}\n",
      "{'train_runtime': 682.3303, 'train_samples_per_second': 38.937, 'train_steps_per_second': 2.436, 'train_loss': 0.036396573740365847, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1662, training_loss=0.036396573740365847, metrics={'train_runtime': 682.3303, 'train_samples_per_second': 38.937, 'train_steps_per_second': 2.436, 'train_loss': 0.036396573740365847, 'epoch': 3.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1970\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088e19e1cea6430dbd4e717031f6538e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1970, 44, 57)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(dataset_all[\"valid\"])\n",
    "print(predictions.predictions.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1970, 44)\n",
      "{'test_loss': 0.16120123863220215, 'test_runtime': 23.3589, 'test_samples_per_second': 84.336, 'test_steps_per_second': 5.308}\n"
     ]
    }
   ],
   "source": [
    "print(predictions.label_ids.shape) # array([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, ...], dtype=int64)\n",
    "print(predictions.metrics)\n",
    "pre = np.argmax(predictions.predictions, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51,  2, 19, ..., 51, 51, 51],\n",
       "       [51, 11, 45, ..., 51, 51, 51],\n",
       "       [51, 11, 28, ..., 51, 51, 51],\n",
       "       ...,\n",
       "       [51,  6, 40, ..., 51, 51, 51],\n",
       "       [51,  2, 36, ..., 51, 51, 51],\n",
       "       [51, 11, 45, ..., 51, 51, 51]], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8671608956393334"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res_src = [ data[1: min(len_matric[i], max_len-2)]\n",
    "                  for i,data in enumerate(pre)]#encode后会加S和E以及padding，label要保证长度一致\n",
    "\n",
    "test_label_src = [ data[1: min(len_matric[i], max_len-2)]\n",
    "                  for i,data in enumerate(predictions.label_ids)]#encode后会加S和E以及padding，label要保证长度一致\n",
    "correct =(np.concatenate(test_res_src,axis=0) == np.concatenate(test_label_src,axis=0)).sum().item()/sum(len_matric)\n",
    "correct                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2e8f04d65c62c49dbc0f2adda76bbd103f82c3255fc54c7e3855bd92e2af645"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
